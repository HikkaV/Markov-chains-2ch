{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.read_csv('data/labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic\n",
       "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0\n",
       "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0\n",
       "2                          Собаке - собачья смерть\\n    1.0\n",
       "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0\n",
       "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = texts.comment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ''.join(i for i in texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Верблюдов-то за что? Дебилы, бл...\\nХохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов е'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, char, n=1, left=True):\n",
    "    if left:\n",
    "        return ' '.join(char for i in range(n)) +\" \" + x\n",
    "    else:\n",
    "        return x+\" \"+ ' '.join(char for i in range(n)) \n",
    "    \n",
    "def process_data(x, start_token='<start>', end_token='<end>', lower=False, n_gram=2):\n",
    "    n_gram = max(n_gram,2)\n",
    "    if isinstance(x, str):\n",
    "        x = re.split(r'[.?!]\\s*',x)\n",
    "    x = [re.sub('[^0-9a-zA-ZА-я]+', ' ', sentence).strip() for sentence in x]\n",
    "    if lower:\n",
    "        x = [sentence.lower() for sentence in x]\n",
    "    x = [sentence.replace('.','').replace(',','').replace('!','').replace('?','').strip() for sentence in x if sentence]\n",
    "    x = [pad(sentence, start_token, n_gram-1, left=True) for sentence in x if sentence]\n",
    "    x = [pad(sentence, end_token, 1, left=False) for sentence in x if sentence]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = process_data(texts, lower=True, n_gram=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> <start> <start> верблюдов то за что <end>',\n",
       " '<start> <start> <start> дебилы бл <end>',\n",
       " '<start> <start> <start> хохлы это отдушина затюканого россиянина мол вон а у хохлов еще хуже <end>',\n",
       " '<start> <start> <start> если бы хохлов не было кисель их бы придумал <end>',\n",
       " '<start> <start> <start> собаке собачья смерть страницу обнови дебил <end>',\n",
       " '<start> <start> <start> это тоже не оскорбление а доказанный факт не дебил про себя во множественном числе писать не будет <end>',\n",
       " '<start> <start> <start> или мы в тебя верим это ты и твои воображаемые друзья <end>',\n",
       " '<start> <start> <start> тебя не убедил 6 страничный пдф в том что скрипалей отравила россия <end>',\n",
       " '<start> <start> <start> анализировать и думать пытаешься <end>',\n",
       " '<start> <start> <start> ватник что ли <end>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_texts [:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChains:\n",
    "    def __init__(self, n_grams=2, texts=None):\n",
    "        self.n_grams = n_grams\n",
    "        self.counter_dict = None\n",
    "        self.n_tokens = None\n",
    "        self.vocab = None\n",
    "        self.inverse_vocab = None\n",
    "        self.texts = texts\n",
    "        if not self.texts is None:\n",
    "            self.counter_dict, self.vocab, self.n_tokens = self.build_vocab()\n",
    "        self.dict_probs = defaultdict(dict)\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        if isinstance(self.texts, list):\n",
    "            vocab = list(set(np.hstack([i.split(' ') for i in self.texts])))\n",
    "        elif isinstance(str): \n",
    "            vocab = list(set(self.texts.split(' ')))\n",
    "            self.texts = [pad(i.strip(), '<end>', left=False) for i in self.texts.split('<end>')]\n",
    "        counter_dict = Counter([])\n",
    "        for sentence in tqdm(processed_texts):\n",
    "            tokens = sentence.split(' ')\n",
    "            for l in range(len(tokens)-self.n_grams+2):\n",
    "                counter_dict.update([' '.join(tokens[l:l+self.n_grams-1])])\n",
    "        n_tokens = len(vocab)\n",
    "        print('Build vocabulary')\n",
    "        return counter_dict, vocab, n_tokens\n",
    "        \n",
    "    def train(self, texts=None):\n",
    "        if texts is None:\n",
    "            assert self.texts, 'Please provide texts data'\n",
    "        else:\n",
    "            self.texts = texts\n",
    "        if self.vocab is None:\n",
    "            self.counter_dict, self.vocab, self.n_tokens = self.build_vocab()\n",
    "        n_gram_counter = Counter([])\n",
    "        for sentence in tqdm(processed_texts):\n",
    "            tokens = sentence.split(' ')\n",
    "            for l in range(len(tokens)-self.n_grams+1):\n",
    "                n_gram_counter.update([' '.join(tokens[l:l+self.n_grams])])\n",
    "        for n_gram, freq in tqdm(n_gram_counter.items()):\n",
    "            tokens = n_gram.split(' ')\n",
    "            main_token = tokens[-1]\n",
    "            cond_tokens = ' '.join(tokens[:-1])\n",
    "            cond_tokens_freq = self.counter_dict[cond_tokens]\n",
    "            self.dict_probs[cond_tokens][main_token] = freq/cond_tokens_freq\n",
    "        print('Finished training')\n",
    "    \n",
    "    def generate(self, start_token='<start>', end_token='<end>', greedy=False):\n",
    "        start_token = [start_token for i in range(self.n_grams-1)]\n",
    "        generated_tokens = [' '.join(start_token)]\n",
    "        next_token = start_token\n",
    "        while True:\n",
    "            generated_tokens = list(np.hstack(i.split(' ') for i in generated_tokens))\n",
    "            probs = self.dict_probs[' '.join(generated_tokens[-self.n_grams+1:])] \n",
    "            variants = list(probs.keys())\n",
    "            probs = list(probs.values())\n",
    "            if greedy:\n",
    "                idx = np.argmax(probs)\n",
    "            else:\n",
    "                idx = np.random.choice(range(len(variants)), p=probs)\n",
    "            next_token = variants[idx]\n",
    "            if next_token==end_token:\n",
    "                break\n",
    "            generated_tokens.append(next_token)\n",
    "        generated_tokens.append('.')\n",
    "        generation = ' '.join(generated_tokens[self.n_grams:]).capitalize()\n",
    "        return generation\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.dict_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35036/35036 [00:01<00:00, 22885.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = MarkovChains(n_grams=4, texts=processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35036/35036 [00:01<00:00, 22899.66it/s]\n",
      "100%|██████████| 389341/389341 [00:01<00:00, 330697.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/volodymyrkovenko/envs/plexai/lib/python3.7/site-packages/ipykernel_launcher.py:53: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ключ переломлен пополам а наш батюшка женя совсем усох он разложился на пряник и на липовый яд поросвещенье вс ид т по плану да кремлеботов как урукхаев из грязи и говна делают .'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
